정적웹데이터 수집 (~/robots.txt 에 사이트마다의 크롤링 허용범위) 
방법1 : request 모듈이용
headers = {'user-agent': 'user-agent 내용(브라우저에서 복사)'}
response = requests.get(url, headers=headers) # url에 한글, 특수문자(' ')
soup = BeautifulSoup(response.text, 'html.parser')

방법2 : urllib.request 모듈이용
url = urllib.parse.quote('한글부분') # 한글입력시 에러남. encoding 해야함.
request = urllib.request.Request(url, headers=headers)
response = urllib.request.urlopen(request) 또는 urlopen(url) 
soup = BeautifulSoup(response, 'html.parser')

soup 함수 = soup.select('선택자') 또는 soup.select_one('선택자')나 soup.find('태그', 속성({dict, class_}) 또는 
					soup.find_all('태그 list', 속성)